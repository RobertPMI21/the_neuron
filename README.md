# Лабораторная работа: Анализ и сравнение функций активации в нейронных сетях для задач классификации

Описание:
Цель данной лабораторной работы заключается в исследовании и сравнении различных функций активации, используемых в нейронных сетях, с фокусом на их эффективности в задачах классификации. В ходе лабораторной работы будут проанализированы следующие функции активации: сигмоид, гиперболический тангенс, ReLU, Leaky ReLU и Parametric ReLU.

Реализация.

Загрузка данных:

Импортируются необходимые библиотеки и модули.
Загружаются данные из набора данных MNIST, который содержит изображения рукописных цифр (0-9).

Предобработка данных:

Изображения вытягиваются в вектор размерности 784 (28x28 пикселей), а затем нормализуются к диапазону [0, 1].
Метки классов преобразуются в one-hot encoding для обеспечения совместимости с задачей классификации.

Определение общих параметров:

input_size - размер входных данных.
output_size - количество классов (цифр от 0 до 9).

Создание и компиляция моделей с разными функциями активации:

Для каждой из пяти моделей определены различные функции активации на скрытых слоях.
Используются полносвязные слои, функции активации и слой softmax на выходе для задачи классификации.

Обучение моделей:

Каждая модель обучается на тренировочных данных с использованием валидационного разделения.
Результаты обучения сохраняются в соответствующих переменных history.


Описание функций активации:

Сигмоида ('sigmoid'):

Преобразует входные значения в диапазон (0, 1).
Часто используется в задачах бинарной классификации.
Гиперболический тангенс ('tanh'):

Преобразует входные значения в диапазон (-1, 1).
Полезен при решении проблемы затухания градиента.

ReLU ('relu'):

Активирует положительные значения, оставляя отрицательные нулевыми.
Эффективен и вычислительно эффективен, но может привести к "умирающим" нейронам.

Leaky ReLU (tf.keras.layers.LeakyReLU(alpha=0.01)):

Вводит небольшой наклон для отрицательных значений, решая проблему "умирающих" нейронов.

Parametric ReLU (PReLU (tf.keras.layers.PReLU)):

Аналог Leaky ReLU, но с обучаемым коэффициентом утечки.

Использование результатов:

Результаты обучения каждой модели могут быть анализированы, чтобы определить, какая функция активации лучше справляется с конкретной задачей классификации на данном наборе данных.




